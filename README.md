# Research Paper Ranking System ğŸ“šğŸ”

A scalable, intelligent research paper search and ranking system that combines multiple information retrieval techniques to deliver highly relevant search results. The system leverages hybrid semantic embeddings, TF-IDF lexical matching, and metadata features to rank academic papers effectively.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [How It Works](#how-it-works)
- [Architecture](#architecture)
- [Current Implementation](#current-implementation)
- [Future Enhancements](#future-enhancements)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Technologies & Techniques](#technologies--techniques)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Contributing](#contributing)

## ğŸ¯ Overview

The Research Paper Ranking System is designed to help researchers, students, and academics quickly find relevant research papers from a large corpus. Given a user query, the system returns the top-ranked papers based on semantic similarity, lexical matching, and various metadata signals such as citations, author count, and references.

**Key Features:**
- **Hybrid Search**: Combines semantic understanding with traditional keyword matching
- **Multi-feature Scoring**: Incorporates citations, references, and author information
- **Scalable Design**: Efficiently processes 1M+ papers using vectorized operations
- **Fast Query Processing**: Pre-computed embeddings and sparse matrices for real-time ranking

## ğŸ”§ How It Works

The system uses a multi-stage ranking pipeline:

### 1. **Query Processing**
   - User enters a natural language query
   - Query is converted into both semantic embeddings and TF-IDF vectors

### 2. **Similarity Computation**
   - **Semantic Similarity**: Compares query embeddings with pre-computed paper embeddings (abstract and title)
   - **Lexical Similarity**: Computes TF-IDF cosine similarity for keyword matching

### 3. **Feature Engineering**
   - Extracts metadata features: citation count, reference count, author count
   - Normalizes features using logarithmic scaling where appropriate

### 4. **Scoring & Ranking**
   - Combines all features using a weighted scoring formula
   - Current weights:
     - Abstract embedding similarity: 35%
     - Title embedding similarity: 25%
     - Combined TF-IDF score: 20%
     - Citation count (log-scaled): 10%
     - Reference count: 5%
     - Author count: 5%

### 5. **Result Delivery**
   - Returns top 20 ranked papers with title, venue, and year

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          USER QUERY                              â”‚
â”‚                     (Natural Language Text)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUERY PROCESSING LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Semantic Embedding  â”‚      â”‚   TF-IDF Vectorizer  â”‚        â”‚
â”‚  â”‚   (MiniLM-L6-v2)    â”‚      â”‚   (Sklearn)          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                â”‚
                         â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SIMILARITY COMPUTATION LAYER                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         PRE-COMPUTED PAPER REPRESENTATIONS               â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚  â€¢ Abstract Embeddings (NumPy)                           â”‚  â”‚
â”‚  â”‚  â€¢ Title Embeddings (NumPy)                              â”‚  â”‚
â”‚  â”‚  â€¢ Abstract TF-IDF Matrix (Sparse)                       â”‚  â”‚
â”‚  â”‚  â€¢ Title TF-IDF Matrix (Sparse)                          â”‚  â”‚
â”‚  â”‚  â€¢ Metadata Features (Parquet)                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  Cosine Similarity Computation:                                 â”‚
â”‚  â€¢ Query â†” Abstract Embeddings                                  â”‚
â”‚  â€¢ Query â†” Title Embeddings                                     â”‚
â”‚  â€¢ Query â†” Abstract TF-IDF                                      â”‚
â”‚  â€¢ Query â†” Title TF-IDF                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCORING & RANKING LAYER                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Final Score = Weighted Combination:                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 0.35 Ã— Abstract_Embedding_Similarity                       â”‚ â”‚
â”‚  â”‚ 0.25 Ã— Title_Embedding_Similarity                          â”‚ â”‚
â”‚  â”‚ 0.20 Ã— Combined_TF-IDF_Score                               â”‚ â”‚
â”‚  â”‚ 0.10 Ã— log(Citation_Count + 1)                             â”‚ â”‚
â”‚  â”‚ 0.05 Ã— Reference_Count                                     â”‚ â”‚
â”‚  â”‚ 0.05 Ã— Author_Count                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚  Sort by Final Score (Descending)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      TOP 20 RANKED PAPERS                        â”‚
â”‚                  (Title, Venue, Year, etc.)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Current Implementation

The current version uses a **Direct Scoring Formula** approach:
- Pre-computed embeddings and TF-IDF matrices are loaded at startup
- Query-time scoring combines multiple signals with fixed weights
- No machine learning model training required for ranking
- Deterministic and interpretable scoring mechanism

### Direct Scoring Formula Details:
```python
final_score = (
    0.35 Ã— abstract_embedding_similarity +
    0.25 Ã— title_embedding_similarity +
    0.20 Ã— combined_tfidf_score +
    0.10 Ã— log(n_citation + 1) +
    0.05 Ã— ref_count +
    0.05 Ã— author_count
)
```

This approach provides:
- âœ… Fast query-time performance
- âœ… Interpretable results
- âœ… No training data requirements
- âœ… Consistent behavior

## ğŸš€ Future Enhancements

The system is designed with extensibility in mind. Planned enhancements include:

### 1. **Learning-to-Rank (LTR) Model**
   - Replace fixed weights with learned parameters
   - Train on user click data and relevance judgments
   - Optimize ranking metrics (NDCG, MRR, MAP)
   - Techniques: LambdaMART, RankNet, or LTR-specific models

### 2. **Document Clustering**
   - Group similar papers into topical clusters
   - Enable cluster-based navigation and exploration
   - Improve diversity in search results
   - Techniques: K-Means, HDBSCAN on embeddings

### 3. **Advanced Retrieval**
   - Approximate Nearest Neighbor (ANN) search using FAISS or Annoy
   - Two-stage retrieval: fast candidate generation + precise ranking
   - Further scalability improvements for 10M+ papers

## ğŸ“ Dataset

This project uses the **Research Papers Dataset** from Kaggle:

ğŸ”— **Dataset Link**: [https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset](https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset)

**Dataset Contents:**
- Research paper titles
- Abstracts
- Authors
- Publication venues
- Publication years
- Citation counts
- References
- And more metadata...

The dataset should be downloaded and processed to generate the required assets (embeddings, TF-IDF matrices, and feature files) stored in the `Core/` directory.

## ğŸ“‚ Project Structure

```
Research-Paper-Ranking-System/
â”‚
â”œâ”€â”€ main.py                          # Main entry point for the ranking system
â”‚
â”œâ”€â”€ Core/                            # Pre-computed assets directory
â”‚   â”œâ”€â”€ Ranking.ipynb               # Jupyter notebook for data preparation
â”‚   â”œâ”€â”€ abstract_embeddings.npy     # Pre-computed abstract embeddings
â”‚   â”œâ”€â”€ title_embeddings.npy        # Pre-computed title embeddings
â”‚   â”œâ”€â”€ X_abstract.npz              # Sparse TF-IDF matrix for abstracts
â”‚   â”œâ”€â”€ X_title.npz                 # Sparse TF-IDF matrix for titles
â”‚   â”œâ”€â”€ abstract_vec.pkl            # Trained abstract TF-IDF vectorizer
â”‚   â”œâ”€â”€ title_vec.pkl               # Trained title TF-IDF vectorizer
â”‚   â”œâ”€â”€ original_data_parquet       # Original dataset in Parquet format
â”‚   â””â”€â”€ training_features_parquet   # Pre-computed features for all papers
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Git ignore rules
â””â”€â”€ README.md                        # This file
```

### Key Files:

- **`main.py`**: The core ranking pipeline that loads assets, processes queries, computes similarities, and returns ranked results.
- **`Core/Ranking.ipynb`**: Jupyter notebook for data preprocessing, generating embeddings, TF-IDF matrices, and feature extraction.
- **Pre-computed Assets**: Binary files (`.npy`, `.npz`, `.pkl`, `.parquet`) containing processed data for fast query-time operations.

## ğŸ› ï¸ Technologies & Techniques

### **Libraries & Frameworks**
- **NumPy**: Efficient numerical computations and array operations
- **Pandas**: Data manipulation and analysis
- **SciPy**: Sparse matrix operations for efficient TF-IDF storage
- **Scikit-learn**: TF-IDF vectorization and cosine similarity
- **Sentence Transformers**: Semantic embeddings using pre-trained models
- **Joblib**: Model serialization and deserialization
- **PyArrow/Fastparquet**: Efficient Parquet file I/O

### **Techniques**
1. **Semantic Embeddings**:
   - Model: `sentence-transformers/all-MiniLM-L6-v2`
   - Captures semantic meaning of text
   - Dense vector representation (384 dimensions)

2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:
   - Traditional lexical matching
   - Sparse matrix representation for memory efficiency
   - Captures keyword importance

3. **Cosine Similarity**:
   - Measures similarity between query and document vectors
   - Efficient vectorized computation

4. **Feature Engineering**:
   - Logarithmic scaling for citation counts (handles skewed distributions)
   - Normalization of metadata features
   - Multi-signal fusion

5. **Hybrid Ranking**:
   - Combines semantic and lexical signals
   - Weighted aggregation of multiple features
   - Balances relevance and popularity

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster embedding generation)
- At least 8GB RAM (16GB+ recommended for large datasets)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/Jai-saraswat/Research-Paper-Ranking-System.git
   cd Research-Paper-Ranking-System
   ```

2. **Create a virtual environment** (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Download the dataset**:
   - Visit [Kaggle Dataset](https://www.kaggle.com/datasets/nechbamohammed/research-papers-dataset)
   - Download the research papers dataset
   - Place it in an accessible location

5. **Generate pre-computed assets**:
   - Open `Core/Ranking.ipynb` in Jupyter Notebook
   - Follow the notebook to:
     - Load and preprocess the dataset
     - Generate embeddings for abstracts and titles
     - Create TF-IDF matrices
     - Extract and save metadata features
   - The notebook will save all assets in the `Core/` directory

### Project Initialization

After generating the assets, your `Core/` directory should contain:
- `abstract_embeddings.npy`
- `title_embeddings.npy`
- `X_abstract.npz`
- `X_title.npz`
- `abstract_vec.pkl`
- `title_vec.pkl`
- `original_data_parquet`
- `training_features_parquet`

## ğŸ’» Usage

### Running the Ranking System

1. **Start the system**:
   ```bash
   python main.py
   ```

2. **Enter your query**:
   ```
   Ask for Research Papers: machine learning applications in healthcare
   ```

3. **View results**:
   The system will display the top 20 ranked papers with their titles, venues, and publication years.

### Example Session

```bash
$ python main.py
Loading Assets...
Assets Loaded.
Ask for Research Papers: deep learning for image classification
Processing Query...
Query Completed.

Top Results:

                                                title                    venue  year
ResNet: Deep Residual Learning for Image Recognition                      CVPR  2015
ImageNet Classification with Deep Convolutional Neural Networks           NIPS  2012
...
```

## ğŸ¤ Contributing

We welcome contributions from the community! Whether you're fixing bugs, adding features, improving documentation, or suggesting enhancements, your help is appreciated.

### How to Contribute

1. **Fork the repository**
2. **Create a feature branch**:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Make your changes**:
   - Write clean, documented code
   - Follow existing code style
   - Add tests if applicable
4. **Commit your changes**:
   ```bash
   git commit -m "Add: description of your changes"
   ```
5. **Push to your fork**:
   ```bash
   git push origin feature/your-feature-name
   ```
6. **Open a Pull Request**:
   - Provide a clear description of your changes
   - Reference any related issues

### Contribution Ideas

- ğŸ¯ Implement Learning-to-Rank models
- ğŸ“Š Add document clustering functionality
- ğŸ” Integrate ANN-based retrieval (FAISS, Annoy)
- ğŸ¨ Create a web interface (Flask/FastAPI + React)
- ğŸ“ˆ Add evaluation metrics and benchmarking
- ğŸ“ Improve documentation and tutorials
- ğŸ› Fix bugs and optimize performance
- âœ… Add unit tests and integration tests

### Code of Conduct

- Be respectful and inclusive
- Provide constructive feedback
- Help others learn and grow

## ğŸ“„ License

This project is open source and available for anyone to use, modify, and contribute to.

## ğŸ“§ Contact

For questions, suggestions, or discussions, feel free to open an issue or reach out through GitHub.

---

**Happy Researching! ğŸ“šâœ¨**
